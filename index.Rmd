---
title: "Statistical and Data Analysis"
subtitle: "for the (social) data scientist"
author: "Austin Hart, Ph.D."
institute: "American University"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, nhsr, nhsr-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---


```{r setup, include = FALSE}
## Libraries
  library(knitr)
  library(rmdformats)

## Global options
  knitr::opts_chunk$set(
    echo = TRUE, 
    prompt = FALSE,
    tidy = FALSE, 
    comment = NA,
    message = FALSE, 
    warning = FALSE,
    dev = 'CairoSVG',
    fig.align = 'center'
)
  
  xaringanExtra::use_panelset()
  options(modelsummary_factory_default = "gt")
```

# Following along

- Fork and clone [AnalysisOverview repo](https://github.com/austin-hart-pols/AnalysisOverview)

- `backloash.dta`: 2016 ANES Survey + 2020 re-interview; white (non-Hispanic) subsample

```{r}
  library(tidyverse); library(haven); library(janitor); library(knitr)
  library(modelsummary); library(lfe); library(kableExtra); library(gt)

## Data
  df = read_dta('backlash.dta') |>
    mutate(
      pid2016 = case_when( # three category PID
        PartyID2016 %in% 1:3 ~ '1. DEM',
        PartyID2016 == 4 ~ '2. IND',
        PartyID2016 %in% 5:7 ~ '3. GOP'
      ),
      Female = as_factor(Female),
      IdeologyLR = na_if(IdeologyLR, 99),
      across(where(is.labelled), ~ as.numeric(.))
    )
```


---
class: inverse, middle

# Counts and frequencies
### For categorical variables

- One-way tables  

- Two-way (contingency) tables

- Test of independence: $\chi^2$  


---
# One-way tables

> Describe distribution of scores on a single, categorical variable.

.pull-left[
### `tidyverse` approach

```{r}
  df |>
    count(pid2016) |>
    na.omit() |>
    mutate(percent = 100 * n / sum(n)) |>
    kable(digits = 1L)
```
]

.pull-right[
### `janitor` alternative

```{r}
  df |>
    tabyl(pid2016, show_na = F) |>
    adorn_totals() |>
    adorn_pct_formatting() |>
    kable(digits = 1L)
```
]

---
# Significant variation?

.pull-left[

### Obs vs Exp
```{r, echo = F}
  df |>
    count(pid2016) |>
    na.omit() |>
    mutate(per = 100 * n / sum(n)) |>
    mutate(
      nExp = 0.5 * sum(n),
      perExp = 100 * nExp / sum(nExp)
    ) |>
    kable(digits = 1L)
```

Is the obs freq significantly different from expected? Find the probability we observe this by chance alone.

Yes, there is significant variation (unequal proportions) across categories $(\chi^2_2 = 640, ~ p < 0.001)$

]

.pull-right[
### Goodness of Fit, $\chi^2$
```{r}
  df |>
    tabyl(pid2016, show_na = F) |>
    pull(n) |>
    stats::chisq.test()
```
]

---
# Contingency (two-way) tables

Describe joint, marginal distribution for two categorical variables.

### Code

```{r, eval = FALSE}
## Raw cross-tab
  t1 = 
    df |>
    tabyl(
      pid2016, Female, # Y, X
      show_na = F,
      show_missing_levels = F
    ) 

## Pretty table
  t1 |>
    adorn_totals(where = c("row", "col")) |>
    adorn_percentages('col') |>
    adorn_pct_formatting(digits = 1) |>
    adorn_title()
```

---
# Contingency (two-way) tables

Describe joint, marginal distribution for two categorical variables.

.pull-left[
### Table Output

```{r, echo = FALSE}
## Raw cross-tab
  t1 = 
    df |>
    tabyl(
      pid2016, Female, # Y, X
      show_na = F,
      show_missing_levels = F
    ) 

## Pretty table
  t1 |>
    adorn_totals(where = c("row", "col")) |>
    adorn_percentages('col') |>
    adorn_pct_formatting(digits = 1) |>
    adorn_title() |>
    kable()
```
]

.pull-right[
### Interpretation
Read down the columns (i.e., within categories of X). 

Among females, 42% identify as Democrats, 45% as Republicans, and about 12% Independent. Among non-females, over half (53%) identify as Republicans, about one third (36%) Democrats, and 12% Indpendent.
]

---
# Contingency (two-way) tables

Describe joint, marginal distribution for two categorical variables.

.pull-left[
### $\chi^2$ Test of independence

Does the distribution of outcomes depend on (i.e., change with) the value of the exposure variable?

```{r}
  chisq.test(t1)
```

]

.pull-right[
Yes. There is evidence of a systematic relationship between party identification and sex.

It is unlikely we observe a pattern like this by chance alone $(\chi^2_2 = 18.9,~p<0.001)$.

*Remember* that the $p-value$ indicates the probability of your sample data given the assumption that party ID and sex are independent (unrelated).
]

---
class: inverse, middle

# Summary statistics
### For numeric variables

- One variable 

- Group comparisons

- $t$-tests

- skew and the $log$-transformation


---
# Describing Distributions
### What to show and tell

- Give context (min to max)

- Cite location (mean)

- Describe variation (standard deviation)

- Describe shape (skew)

- Present a graph


---
# Single continuous variable
### Gather info

.pull-left[
```{r}
## useful stats
  summary(df$ImmIndex)
  sd(df$ImmIndex, na.rm = T)
```
]

.pull-right[
```{r, fig.dim=c(4,3)}
## plots
  hist(df$ImmIndex, main = NULL)
```
]

---
# Immigration Support
### Among 2016 ANES respondents (white, non-Hispanic)

.pull-left[
- Creating the index score
  - Combines responses from 7 questions

  - High scores = pro-immigration  
  
- Scores in 2016 (N = 3,038)

  - *Range of values:* `r min(df$ImmIndex, na.rm = T) |> round(2)` to `r max(df$ImmIndex, na.rm = T) |> round(2)`

  - *Mean*: `r mean(df$ImmIndex, na.rm = T) |> round(2)`

  - *Std Dev*: `r sd(df$ImmIndex, na.rm = T) |> round(2)`

]

.pull-right[

```{r, echo=FALSE, fig.dim=c(4,4)}
df |>
  ggplot(aes(x = ImmIndex)) +
  stat_density(trim = F, fill = 'cornflowerblue') +
  geom_histogram(aes(y = ..density..), color = 'black', fill = NA, bins = 15) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(y = 'Frequency', x = 'Immigration index score') +
  theme_classic(base_size = 14) +
  theme(
    axis.line.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

```

]

---
# Comparing groups
### Subgroup (mean) comparison

.pull-left[
### Subgroup code

```{r, eval = FALSE}
## Group means
  df |>
    group_by(Female) |>
    summarise(
      Avg = mean(ImmIndex, na.rm = T),
      SD = sd(ImmIndex, na.rm = T)
    ) |>
    kable(digits = 2L)

## Compare distributions
  df |>
    ggplot(aes(x = ImmIndex, fill = Female)) +
    stat_density(
      position = 'dodge', 
      alpha = 0.3, color = 'black'
    )
```
]

.pull-right[
### Outputs

```{r, echo = F}
  df |>
    group_by(Female) |>
    summarise(
      Avg = mean(ImmIndex, na.rm = T),
      SD = sd(ImmIndex, na.rm = T)
    ) |>
    kable(digits = 2L)
```

```{r, echo = F, fig.dim=c(4,2.5)}
  df |>
    ggplot(aes(x = ImmIndex, fill = Female)) +
    stat_density(
      position = 'dodge', 
      alpha = 0.3, color = 'black'
    ) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.2))) +
    theme_classic(base_size = 14)
```
]

---
# Mean-focused alternative

.pull-left[
```{r, eval = FALSE}
## Group means
  s1 = df |>
    group_by(pid2016) |>
    summarise(
      Avg = mean(ImmIndex, na.rm =T)
    ) |>
    na.omit()
  
## Plotting
  s1 |>
    ggplot(aes(x = pid2016, y = Avg, 
               color = pid2016)) +
    geom_hline(yintercept = 0) +
    geom_point(size = 3) +
    geom_segment(aes(
      x = pid2016, xend = pid2016, 
      y = 0, yend = Avg
    ))
```
]

.pull-right[
```{r, echo = FALSE, fig.dim=c(4,4)}
## Group means
  df |>
    group_by(pid2016) |>
    summarise(
      Avg = mean(ImmIndex, na.rm =T)
    ) |>
    na.omit() |>
    ggplot(aes(x = pid2016, y = Avg,
               color = fct_rev(pid2016),
               label = round(Avg, 2))) +
    geom_hline(yintercept = 0) +
    geom_point(size = 4) +
    geom_text(hjust = 0, nudge_x = 0.11) +
    geom_segment(
      aes(
        x = pid2016, xend = pid2016, 
        y = 0, yend = Avg
      ),
      size = 1
    ) +
    labs(
      x = NULL,
      y = 'Mean Immig Support'
    ) +
    theme_classic(base_size = 14) +
    theme(
      axis.line = element_blank(),
      axis.ticks = element_blank(),
      axis.text.y = element_blank(),
      legend.position = 'none'
    )
```
]


---
# Testing for differences

.pull-left[
### Diff of (2) means

```{r}
  t.test(ImmIndex ~ Female, df)
```
Support for immigration does not differ systematically for females vs non-females $(t = -0.11, ~ p = 0.91)$.

]

.pull-right[
### ANOVA for more means
```{r}
  summary(aov(ImmIndex ~ pid2016, df))
```


Attitudes about immigration differ significantly by party identification. It is unlikely we observe these differences by chance alone $(F=364, ~p < 0.001)$.

]

---
# What about skew?

.pull-left[
- The Problem   

  - Skewness is the pull of the mean from the median

  - Created by one-sided extreme values

  - Renders mean a poor measure of location/middle

- Solution  

  - Transform the data to mitigate skew
  
  - For positive skew: use $ln(X)$ or $log_{10}(X)$
]

.pull-right[
```{r, echo = FALSE, fig.dim=c(4,4.5)}
  GDP = gapminder::gapminder |>
    filter(year == 1997) |> 
    mutate(
      country = country, 
      gdp = gdpPercap,
      metric = 'Original',
      .keep = 'none'
    )
  G2 = gapminder::gapminder |>
    filter(year == 1997) |> 
    mutate(
      country = country, 
      gdp = log(gdpPercap),
      metric = 'Logged',
      .keep = 'none'
    )
  
  bind_rows(GDP, G2) |>
    ggplot(aes(x = gdp)) +
    facet_wrap(~  fct_rev(metric), nrow = 2, 
               scales = 'free_x', strip.position = 'right') +
    geom_histogram(bins = 15, color = 'white') +
    scale_y_continuous(expand = expansion(mult = c(0, 0.4))) +
    labs(
      y = NULL,
      x = 'Measure of GDP'
    ) +
    theme_classic(base_size = 14) +
    theme(
      axis.line.y = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank()
    )
    
```
]

---
class: inverse, middle

# Linear relationships
### Basic tools

- Correlation

- Regression

### Packages

- `modelsummary` for tables

- `lfe` for analysis of panel data


---
# Linear Correlation

.pull-left[
### Correlation coefficient, $r$

- indexes direction and magnitude of linear dependence

- useful for exploring relationships

- no indication of slope/response

]

.pull-right[
### Implementation in `R`

```{r}
  df |>
    select(IdeologyLR, Income, Resentment, ImmIndex) |>
    corrr::correlate() |>
    kable(digits = 2L, caption = 'Correlation matrix')
```
]



---
# Regression Fundamentals

.panelset[
.panel[.panel-name[Models]

- Regression models specify the relationship between an outcome of interest, $Y_i$, an exposure variable, $X_i$, and a set of covariates, $Z_i$.

- The researcher specifies/imposes the model. 

- Defines the magnitude of "response" in $Y_i$ to changes in $X_i$ among units with the same observed characteristics, $Z_i$. 

- Classic model defines $Y_i$ as linear function of both:

$$
Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + e_i
$$

]

.panel[.panel-name[Lines]


.pull-left[

### The "line"

Expected/predicted value of $Y$:

$$
E[Y_i | X_i, Z_i] = \beta_0 + \beta_1 X_i + \beta_2 Z_i
$$

- $i$ indexes different units

- $Y_i$ is the outcome variable

- $X_i$ is the exposure variable

- $Z_i$ is a control

- The exposure-control distinction is conceptual


]

.pull-right[
### Regression parameters/coefficients

- $\beta_1$: "effect" of treatment holding $Z_i$ constant.
  
- $\beta_2$: impact of $Z_i$, holding $X_i$ constant.

- $\beta_0$: intercept, constant. $E[Y_i | X_i = Z_i = 0]$

]
]

.panel[.panel-name[Error]

.pull-left[
### Defining error

- Specify the relationship: 
$$
Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + e_i
$$

- Observed $Y_i$ equal the combination of linear prediction and error:

$$
Y_i = E[Y_i|X_i,Z_i] + e_i 
$$
- Error equals the difference between observed and expected:
$$
e_i = Y_i - E[Y_i|X_i,Z_i]
$$
]

.pull-right[
### Understanding error 

$e_i$ is the variation in $Y_i$ *not* explained or predicted by $X_i$ and $Z_i$.

$$
error = noise + bias
$$
- noise: randomness in the world; values of $X_i$ and $Z_i$ do not *fully determine* values of $Y_i$.

- bias: systematic influence of excluded causes of $Y_i$.

]
]

.panel[.panel-name[Estimation]

### Ordinary Least Squares (OLS) solution

- Assume the two-variable model: $Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + e_i$

- How do we assign values to parameters (the $\beta$s) given our sample data?

- OLS chooses values that minimize (squared) error, or the total distance between observed and predicted outcomes: 


$$
min \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i + \beta_2 Z_i))^2
$$

]

.panel[.panel-name[Inference]

- We specify the model: $Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + e_i$

- Use sample data to estimate: $Y_i = b_0 + b_1 X_i + b_2 Z_i$

- Test hypothesis

  - Specify null. Typically $\beta_1 = 0$
  
  - Set significance: $\alpha = 0.05$
  
  - Reject null if $Pr(|b_1^*| \geq |b_1| | \beta_1 = 0) \leq 0.05$

]

.panel[.panel-name[Code]

```{r, eval = FALSE}
## Estimate models in a list
  mods = list(
    '(1)' = lm(PartyID2020 ~ ImmIndex + PartyID2016, df),
    '(2)' = lm(PartyID2020 ~ ImmIndex + PartyID2016 + 
              IdeologyLR + Resentment, df),
    '(3)' = lm(PartyID2020 ~ ImmIndex + PartyID2016 + 
              IdeologyLR + Resentment +
                Age + Income, df)
  )

## Generate regression table
  modelsummary(mods, fmt = 2,  gof_map = 'nobs', statistic = NULL, 
               stars = c('*' = 0.05),
               title = 'Immigration attitudes shape white partisanship')
```

]

.panel[.panel-name[Present]

.pull-left[
```{r, echo = F, results = 'asis'}
## Estimate models in a list
  mods = list(
    '(1)' = lm(PartyID2020 ~ ImmIndex + PartyID2016, df),
    '(2)' = lm(PartyID2020 ~ ImmIndex + PartyID2016 + 
              IdeologyLR + Resentment, df),
    '(3)' = lm(PartyID2020 ~ ImmIndex + PartyID2016 + 
              IdeologyLR + Resentment +
                Age + Income, df)
  )

## Generate regression table
  modelsummary(mods, fmt = 2,  gof_map = 'nobs', statistic = NULL, 
               stars = c('*' = 0.05), coef_omit = 'Intercept', 
               title = 'Immigration attitudes shape white partisanship')
```
]

.pull-right[
- Model 1: A one-point increase in support for immigration is associated with a 0.66 shift *away* from the GOP. The effect is significant.

- Estimated impact drops by about half when we control for political ideology and anti-black resentment (2) as well as age and income (3). However the estimate remaisn significant. 

- Note 1: in papers, show standard errors, t-stats, or p-values. Hide in presentations.

- Note 2: changes in coef ests are not a problem! You're mitigating bias in the initial estimates.
]
]

]



---
# Common issues in regression

.panelset[
.panel[.panel-name[DGP + confounds]


- *We specify* a linear model with a single control: 

$$Y_i = \beta_0^* + \beta_1^* X_i + \beta_2^* Z_i + e_i$$

- That's our assumption about the true data generating process (DGP). 

- PROBLEM: We may be wrong. 

- Assume the true DGP is really: 

$$Y_i = \beta_0 + \beta_1 X_i + \beta_2 Z_i + \beta_3 A_i + e_i$$

- Our parameter estimates (the $\beta^*$s) contain bias! It captures impact of $X_i$ AND any unmeasured common causes (confounds).

- SOLUTION: think hard about the DGP, and condition on all common causes of $X$ and $Y$
]

.panel[.panel-name[Cat. predictors]

.pull-left[

- Problem: I have a categorical measure of a predictor variable. Should I treat it as numeric?

- NO! It's categorical; it defines distinct, unordered categories.

- Solution: Use dummy variables! 

  - Divide var into $k$ binary indicators for membership.  
  
  - Include $k-1$ dummies (done automatically). Excluded group is reference.
  
  - Coef represents expected mean diff for included vs reference group.
]

.pull-right[

```{r, echo = F}
## Estimate mods
  mods = list(
    '(1)' = lm(ImmIndex ~ Female, df),
    '(2)' = lm(ImmIndex ~ pid2016, df)
  )

  modelsummary(mods, fmt = 3,  gof_map = 'nobs', statistic = NULL, 
               stars = c('*' = 0.05), 
               coef_map = c('FemaleFemale' = 'Female',
                               'pid20162. IND' = 'Independent',
                               'pid20163. GOP' = 'Republican',
                            '(Intercept)' = '(Intercept)'),
               title = 'Immigration attitudes by gender and party')
```

]
]

.panel[.panel-name[Binary outcomes]

Problem: My DV is binary: $Y = 1$ if respondent is GOP in 2020.

.pull-left[


### Linear Probability Model

- Estimate with OLS. Coefs are expected change in $Pr(Y_I=1)$

- Trouble: straight lines + nonsense predictions

### Logistic Regression

- Estimate: $log(\frac{Pr(Y_i=1)}{1-Pr(Y_i=1)}) = \beta_0 + \beta_1 X_i ...$

- Trouble: interpretation. Coefs are expected change in log odds. Exponentiate to get changes in odds ratio.
]

.pull-right[

```{r, echo=FALSE}
  df = df |> mutate(GOP2020 = if_else(PartyID2020 %in% 5:7, 1, 0))

  mods = list(
    lpm = lm(GOP2020 ~ ImmIndex + IdeologyLR, df),
    logit = glm(GOP2020 ~ ImmIndex + IdeologyLR, df, family = 'binomial')
  )

  modelsummary(mods, fmt = 3,  gof_map = 'nobs', statistic = NULL, 
             stars = c('*' = 0.05), 
             title = 'GOP affilitation in 2020')
  
  # exp(coef(mods$logit))
```

]
]

.panel[.panel-name[More logit]

.pull-left[
- Estimate log-odds: 

$$log(\frac{Pr(Y_i=1)}{1-Pr(Y_i=1)}) = \beta_0 + \beta_1 X_i + \beta_2 Z_i + e_i$$

- $\beta_1$ is expected change in the logit or log-odds of $Y$.


- Or convert to odds-ratios: 

$$\frac{Pr(Y_i=1)}{1-Pr(Y_i=1)} = e^{\beta_0 + \beta_1 X_i + \beta_2 Z_i + e_i}$$
- $e^{\beta_1}$ gives expected factor change in odds of GOP identification.
]

.pull-right[

```{r}
## 2 options
  mods = glm(GOP2020 ~ ImmIndex + IdeologyLR, df, family = 'binomial')
  mods = list('Logit' = mods, 'Odds Ratio' = mods)

## table
  modelsummary(
    mods, fmt = 3, gof_map = 'nobs', statistic = NULL, stars = c('*' = 0.05),
    exponentiate = c(FALSE, TRUE)
  )
```

]

]

]


---
# Regression with panel data

.panelset[
.panel[.panel-name[Panel data]

.pull-left[
- Repeated observations for each unit, typically over time.

$$Y_{i,t} = \beta_{0} + \beta_{1} X_{i,t} + \beta_{2} Z_{i,t} + e_{i}$$

- Leverages two sources of variation: spatial, $i$, and temporal, in $t$ 

- Concerns

  - Must acknowledge repeated obs structure
  
  - Outcomes correlated within units (potentially also across for given year)
  
  - Unmeasured unit-level characteristics
]

.pull-right[

```{r, echo = FALSE}
  gapminder::gapminder |>
  head()

```

]
]

.panel[.panel-name[Fixed Effects]

### Unit-specific intercepts

- Add unique identifiers:

$$Y_{i,t} = \alpha_{i} + \beta_{1} X_{i,t} + \beta_{2} Z_{i,t} + e_{i}$$

- $\alpha_i$ controls for all unique (time-invariant) characteristics of $i$

- Requires that all predictors, $X_i$ etc, vary over time within units.

- Here, $\beta_1$ is a within-unit effect: within $i$, the expected change in $Y_{i,t}$ when $X$ increases.

]

.panel[.panel-name[Code]

.pull-left[
```{r, eval = FALSE}
## Models
  mods = list(
    pooled = lm(lifeExp ~ log(gdpPercap) + log(pop), gapminder::gapminder),
    fe = felm(lifeExp ~ log(gdpPercap) + log(pop) | country, gapminder::gapminder)
  )

  modelsummary(mods, stars = T)
```
]

.pull-right[
```{r, echo = FALSE}
## Models
  mods = list(
    pooled = lm(log(gdpPercap) ~ log(pop) + lifeExp, gapminder::gapminder),
    fe = felm(log(gdpPercap) ~ log(pop) + lifeExp | country, gapminder::gapminder)
  )

    modelsummary(
      mods, fmt = 3,  gof_map = 'nobs', statistic = NULL, 
      stars = c('*' = 0.05), title = 'DV: GDP per captia (log)',
      coef_map = c(
        'log(pop)' = 'Population (log)',
        'lifeExp' = 'Life Expectancy',
        '(Intercept)' = '(Intercept)'
      )
    )
```

Notice how coefficient changes with country fixed effects.

]
]

.panel[.panel-name[Lag X]

.pull-left[

- Problem: $Y$ and $X$ measured simultaneously. Which came first?

- Solution: use the prior value $X_{i,t-1}$ to predict $Y$.

```{r}
## Create lags
  ld = gapminder::gapminder |>
    group_by(country) |>
    mutate(
      poplag = lag(pop, order_by = year),
      lelag = lag(lifeExp, order_by = year)
    )
```
]



.pull-right[
```{r, echo=FALSE}
## Models
  mods = list(
    pooled = lm(log(gdpPercap) ~ log(poplag) + lelag, ld),
    fe = felm(log(gdpPercap) ~ log(poplag) + lelag | country, ld)
  )

    modelsummary(
      mods, fmt = 3,  gof_map = 'nobs', statistic = NULL, 
      stars = c('*' = 0.05),
      output = 'gt',
      notes = 'Note: DV: GDP per captia (log). Predictors lagged 5 years.',
      coef_map = c(
        'log(poplag)' = 'Population (log)',
        'lelag' = 'Life Expectancy',
        '(Intercept)' = '(Intercept)'
      )
    )
```
]
]
]


